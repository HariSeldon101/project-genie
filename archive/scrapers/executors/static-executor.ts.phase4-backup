/**
 * Static Scraper Executor
 * 
 * EXTENDS BaseScraperExecutor (DRY principle applied)
 * Fast HTML scraping using Cheerio (no browser required).
 * Perfect for static sites, server-rendered content, and initial discovery.
 * 
 * COMPREHENSIVE LOGGING: 30+ logging points with breadcrumbs
 * ERROR PROPAGATION: All errors sent to UI via SSEEventFactory
 * NO SILENT FAILURES: Every error is logged and propagated
 * 
 * @module static-executor
 */

import * as cheerio from 'cheerio'
import {
  ScraperResult,
  ScraperOptions,
  PageResult,
  ValidationResult,
  MergedPageData,
  ValueEstimate,
  ScrapingError,
  DiscoveredLink,
  ContactInfo,
  SocialLinks,
  FormData,
  ImageData
} from '../additive/types'
import { permanentLogger } from '@/lib/utils/permanent-logger'
import { BaseScraperExecutor, ProgressCallback } from './base-executor'
import { SSEEventFactory, EventSource } from '@/lib/company-intelligence/utils/sse-event-factory'
import { SmartExtractor } from '@/lib/company-intelligence/extractors/smart-extractor'
import { BlogContentExtractor } from '@/lib/company-intelligence/extractors/blog-content-extractor'
import { socialMediaExtractor } from '../extractors/social-media-extractor'

/**
 * Static HTML scraper using Cheerio
 * Extends BaseScraperExecutor for DRY principle
 */
export class StaticScraperExecutor extends BaseScraperExecutor {
  id = 'static'
  name = 'Static HTML Scraper (Cheerio)'
  strategy = 'static' as const
  description = 'Fast HTML parsing without JavaScript execution. Best for static sites, blogs, and server-rendered content.'
  speed = 'fast' as const
  
  private smartExtractor: SmartExtractor
  private blogExtractor: BlogContentExtractor
  private socialExtractor: typeof socialMediaExtractor

  constructor() {
    super()
    this.smartExtractor = new SmartExtractor()
    this.blogExtractor = new BlogContentExtractor()
    this.socialExtractor = socialMediaExtractor
  }

  /**
   * Execute static scraping on URLs with comprehensive logging
   */
  async scrape(urls: string[], options?: ScraperOptions): Promise<ScraperResult> {
    const startTime = Date.now()
    const pages: PageResult[] = []
    const allDiscoveredLinks: DiscoveredLink[] = []
    const errors: ScrapingError[] = []
    
    // CRITICAL DEBUG: Check if URLs are being passed correctly
    console.log('ðŸ”´ STATIC_EXECUTOR.scrape CALLED:', {
      urlsReceived: !!urls,
      urlCount: urls ? urls.length : 0,
      urlsSample: urls ? urls.slice(0, 3) : [],
      urlsType: Array.isArray(urls) ? 'array' : typeof urls,
      hasOptions: !!options,
      sessionId: options?.sessionId
    })
    
    // CRITICAL: Check for empty URLs array - THROW ERROR, NO FALLBACKS!
    if (!urls || urls.length === 0) {
      // Log comprehensive error with forensic data
      permanentLogger.error('STATIC_EXECUTOR', 'No URLs provided for scraping', {
        breadcrumbs: permanentLogger.getBreadcrumbs(),
        timestamp: Date.now(),
        sessionId: options?.sessionId,
        urlsReceived: urls,
        urlsType: Array.isArray(urls) ? 'array' : typeof urls,
        source: 'static_executor_scrape',
        systemState: {
          hasOptions: !!options,
          optionKeys: options ? Object.keys(options) : []
        }
      })
      
      // Send error to UI via SSE if callback provided
      if (options?.progressCallback) {
        const errorEvent = SSEEventFactory.error(
          new Error('No URLs provided for scraping - check sitemap discovery phase'),
          {
            source: EventSource.SCRAPER,
            phase: 'initialization',
            correlationId: options.sessionId
          }
        )
        
        try {
          await options.progressCallback(errorEvent)
        } catch (sseError) {
          permanentLogger.error('STATIC_EXECUTOR', 'Failed to send SSE error event', {
            error: sseError instanceof Error ? sseError.message : String(sseError)
          })
        }
      }
      
      // Throw descriptive error - NO FALLBACK!
      const error = new Error(
        'No URLs provided for scraping. Please ensure sitemap discovery has completed and discovered_urls contains valid URLs.'
      )
      
      // Add additional context to error
      (error as any).context = {
        sessionId: options?.sessionId,
        timestamp: Date.now(),
        source: 'StaticScraperExecutor',
        phase: 'initialization'
      }
      
      throw error
    }
    
    permanentLogger.breadcrumb('static_scraper_init', 'Static scraper initializing', {
      urlCount: urls.length,
      maxPages: options?.maxPages,
      timeout: options?.timeout
    })
    
    // Initialize session with base class logging
    await this.startScrapingSession(urls, options)
    
    permanentLogger.log('STATIC_EXECUTOR', 'Starting static HTML scraping', {
      urlCount: urls.length,
      strategy: this.strategy,
      speed: this.speed,
      userAgent: options?.userAgent,
      hasHeaders: !!options?.headers,
      extractTypes: options?.extractTypes
    })

    // CRITICAL DEBUG: About to start processing URLs
    console.log('ðŸ”´ STATIC_EXECUTOR: About to process URLs in loop:', {
      urlCount: urls.length,
      firstUrl: urls[0],
      lastUrl: urls[urls.length - 1]
    })

    // Process URLs with detailed logging for each page
    let processedCount = 0
    for (const url of urls) {
      console.log(`ðŸ”´ STATIC_EXECUTOR: Processing URL ${processedCount + 1}/${urls.length}: ${url}`)
      try {
        permanentLogger.breadcrumb('page_scrape_start', `Starting scrape of ${url}`, {
          url,
          index: processedCount,
          total: urls.length
        })
        
        // Log page attempt
        this.logPageAttempt(url, processedCount, urls.length)
        
        // Send progress event
        await this.sendProgressEvent(
          processedCount,
          urls.length,
          `Scraping ${url.substring(0, 50)}...`
        )
        
        const pageStartTime = Date.now()
        const pageResult = await this.scrapePage(url, options)
        const pageDuration = Date.now() - pageStartTime
        
        pages.push(pageResult)
        
        if (pageResult.success) {
          // Log successful scrape
          await this.logPageSuccess(url, pageResult, pageDuration)
          
          // Collect discovered links with metadata
          if (pageResult.discoveredLinks) {
            permanentLogger.breadcrumb('links_discovered', `Found ${pageResult.discoveredLinks.length} links`, {
              url,
              linkCount: pageResult.discoveredLinks.length
            })
            
            for (const link of pageResult.discoveredLinks) {
              allDiscoveredLinks.push({
                url: link,
                foundOn: url,
                type: this.classifyLink(link, url),
                scraped: false
              })
            }
          }
        } else {
          // Log page error
          await this.logPageError(url, pageResult.error || 'Unknown error', pageDuration)
        }
        
        processedCount++
        
      } catch (error) {
        const errorMessage = error instanceof Error ? error.message : String(error) || 'Unknown error'
        
        permanentLogger.error('STATIC_EXECUTOR', 'Critical error scraping page', {
          url,
          error: errorMessage,
          stack: error instanceof Error ? error.stack : undefined,
          processedCount,
          totalUrls: urls.length
        })
        
        // Log page error
        await this.logPageError(url, error, Date.now() - startTime)
        
        errors.push({
          code: 'SCRAPE_ERROR',
          message: errorMessage,
          url,
          stack: error instanceof Error ? error.stack : undefined,
          timestamp: Date.now()
        })
        
        pages.push({
          url,
          success: false,
          error: errorMessage
        })
        
        processedCount++
      }
    }

    // Calculate statistics using base class
    this.stats.duration = Date.now() - startTime
    this.stats.pagesAttempted = pages.length
    this.stats.pagesSucceeded = pages.filter(p => p.success).length
    this.stats.pagesFailed = pages.filter(p => !p.success).length
    this.stats.bytesDownloaded = pages.reduce((sum, p) => sum + (p.bytesDownloaded || 0), 0)
    this.stats.linksDiscovered = allDiscoveredLinks.length
    this.stats.averageTimePerPage = this.stats.pagesAttempted > 0 
      ? Math.round(this.stats.duration / this.stats.pagesAttempted) 
      : 0
    this.stats.successRate = this.stats.pagesAttempted > 0
      ? Math.round((this.stats.pagesSucceeded / this.stats.pagesAttempted) * 100)
      : 0
    
    // Count total data points
    for (const page of pages) {
      if (!page.success) continue
      let dataPoints = 0
      if (page.title) dataPoints++
      if (page.description) dataPoints++
      if (page.structuredData && Object.keys(page.structuredData).length > 0) {
        dataPoints += Object.keys(page.structuredData).length
      }
      if (page.technologies?.length) dataPoints += page.technologies.length
      if (page.contactInfo?.emails?.length) dataPoints += page.contactInfo.emails.length
      if (page.contactInfo?.phones?.length) dataPoints += page.contactInfo.phones.length
      if (page.socialLinks && Object.keys(page.socialLinks).length > 0) {
        dataPoints += Object.keys(page.socialLinks).length
      }
      if (page.forms?.length) dataPoints += page.forms.length
      if (page.discoveredLinks?.length) dataPoints += page.discoveredLinks.length
      this.stats.dataPointsExtracted += dataPoints
    }
    
    // Generate suggestions based on findings
    const suggestions = this.generateSuggestions(pages, allDiscoveredLinks)
    
    const result: ScraperResult = {
      scraperId: this.id,
      scraperName: this.name,
      strategy: this.strategy,
      timestamp: Date.now(),
      pages,
      discoveredLinks: allDiscoveredLinks,
      stats: this.stats,
      errors,
      suggestions
    }
    
    // Complete session with base class logging
    await this.completeScrapingSession(result, Date.now() - startTime)
    
    permanentLogger.log('STATIC_EXECUTOR', 'Static scraping completed', {
      duration: this.stats.duration,
      pagesSucceeded: this.stats.pagesSucceeded,
      pagesFailed: this.stats.pagesFailed,
      dataPointsExtracted: this.stats.dataPointsExtracted,
      linksDiscovered: allDiscoveredLinks.length,
      errors: errors.length,
      suggestions: suggestions.length
    })
    
    return result
  }

  /**
   * Scrape a single page with comprehensive logging
   */
  private async scrapePage(url: string, options?: ScraperOptions): Promise<PageResult> {
    const startTime = Date.now()
    
    permanentLogger.breadcrumb('fetch_start', `Fetching HTML for ${url}`, {
      url,
      timeout: options?.timeout,
      hasHeaders: !!options?.headers
    })
    
    try {
      // Fetch the HTML
      const response = await fetch(url, {
        headers: options?.headers || {
          'User-Agent': options?.userAgent || 'Mozilla/5.0 (compatible; ProjectGenie/1.0)'
        },
        signal: options?.timeout ? AbortSignal.timeout(options.timeout) : undefined
      })
      
      permanentLogger.breadcrumb('fetch_complete', `Received response for ${url}`, {
        url,
        status: response.status,
        statusText: response.statusText,
        contentType: response.headers.get('content-type')
      })
      
      if (!response.ok) {
        const errorMessage = `HTTP ${response.status}: ${response.statusText}`
        
        // Handle 404s gracefully - mark page as unavailable but don't fail entire scraping
        if (response.status === 404) {
          permanentLogger.log('STATIC_EXECUTOR', 'Page not found (404) - returning empty result', { 
            url,
            status: response.status 
          })
          
          return {
            url,
            success: false,
            statusCode: response.status,
            error: '404 - Page not found',
            title: '',
            description: '',
            textContent: '',
            discoveredLinks: [],
            structuredData: {},
            technologies: [],
            apiEndpoints: [],
            contactInfo: {},
            socialLinks: {},
            forms: [],
            images: [],
            duration: Date.now() - startTime,
            bytesDownloaded: 0
          }
        }
        
        // For other errors, log and throw
        permanentLogger.error('STATIC_EXECUTOR', 'HTTP error fetching page', {
          url,
          status: response.status,
          statusText: response.statusText,
          errorMessage
        })
        
        throw new Error(errorMessage)
      }
      
      const html = await response.text()
      const htmlSize = new TextEncoder().encode(html).length
      
      permanentLogger.breadcrumb('html_received', `HTML received for ${url}`, {
        url,
        htmlSize,
        htmlLength: html.length
      })
      
      // Parse HTML with Cheerio
      const $ = cheerio.load(html)
      
      permanentLogger.breadcrumb('extraction_start', `Extracting data from ${url}`, {
        url
      })
      
      // Extract all data using both extractors and existing methods
      // Step 1: Use SmartExtractor for comprehensive company data
      const companyData = this.smartExtractor.extractFromHtml(html, url)
      permanentLogger.breadcrumb('smart_extraction_complete', 'SmartExtractor completed', {
        hasCompanyName: !!companyData.basics.companyName,
        hasDescription: !!companyData.basics.description,
        hasTagline: !!companyData.basics.tagline,
        productCount: companyData.business.products?.length || 0,
        serviceCount: companyData.business.services?.length || 0,
        teamSize: companyData.team.size,
        keyPeopleCount: companyData.team.keyPeople?.length || 0
      })
      
      // Step 2: Extract basic page data from extractors
      const title = companyData.basics.companyName || $('title').first().text().trim() || $('h1').first().text().trim()
      permanentLogger.breadcrumb('title_extracted', 'Title extracted', { title: title?.substring(0, 100) })
      
      const description = companyData.basics.description || 
        $('meta[name="description"]').attr('content') || 
        $('meta[property="og:description"]').attr('content') || ''
      permanentLogger.breadcrumb('description_extracted', 'Description extracted', { descLength: description?.length })
      
      // Extract text content directly (simplified version)
      const textContent = $('body').text().replace(/\s+/g, ' ').trim().substring(0, 50000)
      permanentLogger.breadcrumb('text_extracted', 'Text content extracted', { textLength: textContent?.length })
      
      // Step 3: Check if this is a blog page and use BlogContentExtractor
      const isBlogPage = url.includes('/blog') || url.includes('/news') || url.includes('/articles') || 
                        url.includes('/posts') || url.includes('/insights')
      let blogLinks: string[] = []
      
      if (isBlogPage) {
        blogLinks = this.blogExtractor.extractBlogLinks(html, url)
        permanentLogger.breadcrumb('blog_links_extracted', 'Blog-specific links extracted', { 
          blogLinkCount: blogLinks.length,
          isBlogPage: true 
        })
      }
      
      // Step 4: Extract all links directly
      const discoveredLinks: string[] = []
      $('a[href]').each((_, element) => {
        const href = $(element).attr('href')
        if (href) {
          try {
            const absoluteUrl = new URL(href, url).toString()
            if (absoluteUrl.startsWith('http')) {
              discoveredLinks.push(absoluteUrl)
            }
          } catch {
            // Invalid URL, skip
          }
        }
      })
      
      // Merge blog links with discovered links (avoiding duplicates)
      const allDiscoveredLinks = Array.from(new Set([...discoveredLinks, ...blogLinks]))
      permanentLogger.breadcrumb('links_extracted', 'All links extracted and merged', { 
        totalLinkCount: allDiscoveredLinks.length,
        blogLinkCount: blogLinks.length,
        regularLinkCount: discoveredLinks.length 
      })
      
      const structuredData = this.extractStructuredData($)
      permanentLogger.breadcrumb('structured_data_extracted', 'Structured data extracted', { 
        hasStructuredData: Object.keys(structuredData).length > 0,
        types: Object.keys(structuredData)
      })
      
      const technologies = this.detectTechnologies($, html)
      permanentLogger.breadcrumb('technologies_detected', 'Technologies detected', { technologies })
      
      const apiEndpoints = this.extractAPIEndpoints($, html)
      permanentLogger.breadcrumb('api_endpoints_extracted', 'API endpoints extracted', { 
        endpointCount: apiEndpoints.length 
      })
      
      // Step 5: Use SmartExtractor data for contact info
      const contactInfo = {
        emails: companyData.basics.email ? [companyData.basics.email] : [],
        phones: companyData.basics.phone ? [companyData.basics.phone] : [],
        addresses: [] // SmartExtractor doesn't extract addresses yet
      }
      permanentLogger.breadcrumb('contact_info_extracted', 'Contact info extracted and enhanced', {
        emailCount: contactInfo.emails?.length || 0,
        phoneCount: contactInfo.phones?.length || 0,
        addressCount: contactInfo.addresses?.length || 0,
        fromSmartExtractor: !!(companyData.basics.email || companyData.basics.phone)
      })
      
      // Step 6: Use SmartExtractor social media data
      const socialLinks = companyData.basics.socialMedia || {}
      permanentLogger.breadcrumb('social_links_extracted', 'Social links extracted', {
        platforms: Object.keys(socialLinks)
      })
      
      const forms = this.extractForms($)
      permanentLogger.breadcrumb('forms_extracted', 'Forms extracted', { formCount: forms.length })
      
      const images = this.extractImages($, url)
      permanentLogger.breadcrumb('images_extracted', 'Images extracted', { imageCount: images.length })
      
      const duration = Date.now() - startTime
      
      permanentLogger.log('STATIC_EXECUTOR', 'Page scraped successfully', {
        url,
        success: true,
        duration,
        statusCode: response.status,
        bytesDownloaded: htmlSize,
        title: title?.substring(0, 100),
        hasDescription: !!description,
        textLength: textContent?.length,
        linkCount: discoveredLinks.length,
        techCount: technologies.length,
        apiCount: apiEndpoints.length,
        formCount: forms.length,
        imageCount: images.length
      })
      
      return {
        url,
        success: true,
        statusCode: response.status,
        title,
        description,
        content: html,
        textContent,
        discoveredLinks: allDiscoveredLinks,
        structuredData,
        technologies,
        apiEndpoints,
        contactInfo,
        socialLinks,
        forms,
        images,
        duration,
        bytesDownloaded: htmlSize,
        // New data from extractors
        companyData,
        blogLinks,
        isBlogPage
      }
      
    } catch (error) {
      const errorMessage = error instanceof Error ? error.message : String(error)
      const duration = Date.now() - startTime
      
      permanentLogger.error('STATIC_EXECUTOR', 'Failed to scrape page', {
        url,
        error: errorMessage,
        stack: error instanceof Error ? error.stack : undefined,
        duration
      })
      
      return {
        url,
        success: false,
        error: errorMessage,
        duration
      }
    }
  }

  /**
   * Extract page title
   */
  private extractTitle($: cheerio.CheerioAPI): string {
    return $('title').first().text().trim() ||
           $('meta[property="og:title"]').attr('content')?.trim() ||
           $('h1').first().text().trim() ||
           ''
  }

  /**
   * Extract page description
   */
  private extractDescription($: cheerio.CheerioAPI): string {
    return $('meta[name="description"]').attr('content')?.trim() ||
           $('meta[property="og:description"]').attr('content')?.trim() ||
           $('p').first().text().trim().substring(0, 200) ||
           ''
  }

  /**
   * Extract text content
   */
  private extractTextContent($: cheerio.CheerioAPI): string {
    // Remove script and style elements
    $('script, style, noscript').remove()
    
    // Get main content areas
    const contentSelectors = [
      'main',
      'article',
      '[role="main"]',
      '#content',
      '.content',
      'body'
    ]
    
    for (const selector of contentSelectors) {
      const content = $(selector).first()
      if (content.length) {
        return content.text().replace(/\s+/g, ' ').trim()
      }
    }
    
    return $('body').text().replace(/\s+/g, ' ').trim()
  }

  /**
   * Extract all links from the page with proper deduplication
   */
  private extractLinks($: cheerio.CheerioAPI, baseUrl: string): string[] {
    const links: string[] = []
    const seen = new Set<string>()
    
    // Extract all links from <a> tags
    $('a[href]').each((_, element) => {
      const href = $(element).attr('href')
      if (!href) return
      
      // Skip non-HTTP links
      if (href.startsWith('javascript:') || 
          href.startsWith('mailto:') || 
          href.startsWith('tel:') ||
          href.startsWith('#')) {
        return
      }
      
      // Resolve relative URLs
      let fullUrl = href
      try {
        const url = new URL(href, baseUrl)
        fullUrl = url.href
        
        // Skip non-HTTP(S) protocols
        if (url.protocol !== 'http:' && url.protocol !== 'https:') {
          return
        }
      } catch {
        // If URL parsing fails, skip this link
        return
      }
      
      // Skip if already seen
      if (seen.has(fullUrl)) return
      
      // Skip asset files
      if (/\.(css|js|jpg|jpeg|png|gif|svg|ico|woff|woff2|ttf|eot|pdf|zip|doc|docx|xls|xlsx)$/i.test(fullUrl)) {
        return
      }
      
      // Add to seen set and links array
      seen.add(fullUrl)
      links.push(fullUrl)
    })
    
    // Also include canonical and alternate links
    $('link[rel="canonical"], link[rel="alternate"]').each((_, element) => {
      const href = $(element).attr('href')
      if (!href) return
      
      try {
        const url = new URL(href, baseUrl)
        const fullUrl = url.href
        
        if (!seen.has(fullUrl) && 
            !(/\.(css|js|jpg|jpeg|png|gif|svg|ico|woff|woff2|ttf|eot|pdf|zip|doc|docx|xls|xlsx)$/i.test(fullUrl))) {
          seen.add(fullUrl)
          links.push(fullUrl)
        }
      } catch {
        // Invalid URL, skip
      }
    })
    
    return links
  }

  /**
   * Extract structured data (JSON-LD, microdata)
   */
  private extractStructuredData($: cheerio.CheerioAPI): Record<string, any> {
    const structured: Record<string, any> = {}
    
    // Extract JSON-LD
    $('script[type="application/ld+json"]').each((_, element) => {
      try {
        const data = JSON.parse($(element).html() || '{}')
        if (data['@type']) {
          structured[data['@type']] = data
        }
      } catch {
        // Invalid JSON, skip
      }
    })
    
    // Extract Open Graph data
    const og: Record<string, string> = {}
    $('meta[property^="og:"]').each((_, element) => {
      const property = $(element).attr('property')?.replace('og:', '')
      const content = $(element).attr('content')
      if (property && content) {
        og[property] = content
      }
    })
    if (Object.keys(og).length > 0) {
      structured.openGraph = og
    }
    
    // Extract Twitter Card data
    const twitter: Record<string, string> = {}
    $('meta[name^="twitter:"]').each((_, element) => {
      const name = $(element).attr('name')?.replace('twitter:', '')
      const content = $(element).attr('content')
      if (name && content) {
        twitter[name] = content
      }
    })
    if (Object.keys(twitter).length > 0) {
      structured.twitterCard = twitter
    }
    
    return structured
  }

  /**
   * Detect technologies used
   */
  private detectTechnologies($: cheerio.CheerioAPI, html: string): string[] {
    const technologies = new Set<string>()
    
    // Check meta generators
    const generator = $('meta[name="generator"]').attr('content')
    if (generator) {
      technologies.add(generator.split(' ')[0])
    }
    
    // Check for common frameworks in HTML
    const techPatterns = [
      { pattern: /wp-content|wordpress/i, name: 'WordPress' },
      { pattern: /shopify/i, name: 'Shopify' },
      { pattern: /_next|next\.js|nextjs/i, name: 'Next.js' },
      { pattern: /gatsby/i, name: 'Gatsby' },
      { pattern: /react/i, name: 'React' },
      { pattern: /vue\.js|vuejs/i, name: 'Vue.js' },
      { pattern: /angular/i, name: 'Angular' },
      { pattern: /bootstrap/i, name: 'Bootstrap' },
      { pattern: /tailwind/i, name: 'Tailwind CSS' },
      { pattern: /jquery/i, name: 'jQuery' },
      { pattern: /cloudflare/i, name: 'Cloudflare' },
      { pattern: /google\s*analytics|gtag/i, name: 'Google Analytics' },
      { pattern: /facebook\s*pixel/i, name: 'Facebook Pixel' }
    ]
    
    for (const { pattern, name } of techPatterns) {
      if (pattern.test(html)) {
        technologies.add(name)
      }
    }
    
    // Check script sources
    $('script[src]').each((_, element) => {
      const src = $(element).attr('src') || ''
      
      if (src.includes('google-analytics.com')) technologies.add('Google Analytics')
      if (src.includes('googletagmanager.com')) technologies.add('Google Tag Manager')
      if (src.includes('facebook.com')) technologies.add('Facebook SDK')
      if (src.includes('cloudflare.com')) technologies.add('Cloudflare')
      if (src.includes('cdn.shopify.com')) technologies.add('Shopify')
    })
    
    return Array.from(technologies)
  }

  /**
   * Extract API endpoints
   */
  private extractAPIEndpoints($: cheerio.CheerioAPI, html: string): string[] {
    const endpoints = new Set<string>()
    
    // Look for API URLs in scripts
    const apiPatterns = [
      /["'](\/api\/[^"']+)["']/g,
      /["'](https?:\/\/[^"']*\/api\/[^"']+)["']/g,
      /["'](\/v\d+\/[^"']+)["']/g,
      /["'](https?:\/\/api\.[^"']+)["']/g
    ]
    
    for (const pattern of apiPatterns) {
      const matches = html.matchAll(pattern)
      for (const match of matches) {
        endpoints.add(match[1])
      }
    }
    
    // Look for data attributes that might contain API endpoints
    $('[data-api-url], [data-api-endpoint], [data-url]').each((_, element) => {
      const url = $(element).attr('data-api-url') || 
                  $(element).attr('data-api-endpoint') || 
                  $(element).attr('data-url')
      if (url && (url.includes('/api/') || url.includes('/v1/') || url.includes('/v2/'))) {
        endpoints.add(url)
      }
    })
    
    return Array.from(endpoints)
  }

  /**
   * Extract contact information
   */
  private extractContactInfo($: cheerio.CheerioAPI): ContactInfo {
    const info: ContactInfo = {
      emails: [],
      phones: [],
      addresses: []
    }
    
    // Extract emails
    const emailPattern = /\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b/g
    const text = $('body').text()
    const emailMatches = text.match(emailPattern) || []
    info.emails = Array.from(new Set(emailMatches))
    
    // Also check mailto links
    $('a[href^="mailto:"]').each((_, element) => {
      const mailto = $(element).attr('href')?.replace('mailto:', '').split('?')[0]
      if (mailto && !info.emails?.includes(mailto)) {
        info.emails?.push(mailto)
      }
    })
    
    // Extract phone numbers
    const phonePattern = /[\+]?[(]?[0-9]{1,4}[)]?[-\s\.]?[(]?[0-9]{1,4}[)]?[-\s\.]?[0-9]{1,4}[-\s\.]?[0-9]{1,4}/g
    const phoneMatches = text.match(phonePattern) || []
    info.phones = Array.from(new Set(phoneMatches.filter(p => p.length >= 10)))
    
    // Also check tel links
    $('a[href^="tel:"]').each((_, element) => {
      const tel = $(element).attr('href')?.replace('tel:', '')
      if (tel && !info.phones?.includes(tel)) {
        info.phones?.push(tel)
      }
    })
    
    // Extract addresses
    const addressSelectors = [
      'address',
      '[itemprop="address"]',
      '.address',
      '[class*="address"]'
    ]
    
    const addresses = new Set<string>()
    for (const selector of addressSelectors) {
      $(selector).each((_, element) => {
        const address = $(element).text().trim()
        if (address && address.length > 10) {
          addresses.add(address)
        }
      })
    }
    info.addresses = Array.from(addresses)
    
    return info
  }

  /**
   * Extract social media links
   */
  private extractSocialLinks($: cheerio.CheerioAPI): SocialLinks {
    const social: SocialLinks = {}
    
    const socialPatterns = [
      { platform: 'twitter', pattern: /twitter\.com|x\.com/ },
      { platform: 'linkedin', pattern: /linkedin\.com/ },
      { platform: 'facebook', pattern: /facebook\.com/ },
      { platform: 'instagram', pattern: /instagram\.com/ },
      { platform: 'youtube', pattern: /youtube\.com/ },
      { platform: 'github', pattern: /github\.com/ },
      { platform: 'tiktok', pattern: /tiktok\.com/ },
      { platform: 'pinterest', pattern: /pinterest\.com/ }
    ]
    
    $('a[href]').each((_, element) => {
      const href = $(element).attr('href')
      if (!href) return
      
      for (const { platform, pattern } of socialPatterns) {
        if (pattern.test(href) && !social[platform]) {
          social[platform] = href
        }
      }
    })
    
    return social
  }

  /**
   * Extract forms
   */
  private extractForms($: cheerio.CheerioAPI): FormData[] {
    const forms: FormData[] = []
    
    $('form').each((_, element) => {
      const $form = $(element)
      const fields: FormData['fields'] = []
      
      $form.find('input, select, textarea').each((_, field) => {
        const $field = $(field)
        fields.push({
          name: $field.attr('name'),
          type: $field.attr('type') || $field.prop('tagName')?.toLowerCase(),
          required: $field.attr('required') !== undefined
        })
      })
      
      forms.push({
        action: $form.attr('action'),
        method: $form.attr('method'),
        fields
      })
    })
    
    return forms
  }

  /**
   * Extract images
   */
  private extractImages($: cheerio.CheerioAPI, baseUrl: string): ImageData[] {
    const images: ImageData[] = []
    const seen = new Set<string>()
    
    $('img').each((_, element) => {
      const $img = $(element)
      let src = $img.attr('src') || $img.attr('data-src')
      
      if (!src || seen.has(src)) return
      
      // Resolve relative URLs
      try {
        const url = new URL(src, baseUrl)
        src = url.href
      } catch {
        // Keep as is if invalid
      }
      
      seen.add(src)
      
      images.push({
        src,
        alt: $img.attr('alt'),
        width: parseInt($img.attr('width') || '0'),
        height: parseInt($img.attr('height') || '0')
      })
    })
    
    return images
  }

  /**
   * Classify a link type
   */
  private classifyLink(link: string, sourceUrl: string): DiscoveredLink['type'] {
    try {
      const linkUrl = new URL(link)
      const sourceUrlObj = new URL(sourceUrl)
      
      // Check if same domain
      if (linkUrl.hostname === sourceUrlObj.hostname) {
        // Check if it's an asset
        if (/\.(jpg|jpeg|png|gif|svg|css|js|pdf|doc|docx|xls|xlsx|zip)$/i.test(linkUrl.pathname)) {
          return 'asset'
        }
        // Check if it's an API endpoint
        if (linkUrl.pathname.includes('/api/') || linkUrl.pathname.includes('/v1/') || linkUrl.pathname.includes('/v2/')) {
          return 'api'
        }
        return 'internal'
      }
      
      // Check if it's a social media link
      if (/twitter\.com|facebook\.com|linkedin\.com|instagram\.com|youtube\.com|github\.com/i.test(linkUrl.hostname)) {
        return 'social'
      }
      
      return 'external'
    } catch {
      return 'external'
    }
  }

  /**
   * Generate suggestions based on findings
   */
  private generateSuggestions(pages: PageResult[], links: DiscoveredLink[]): any[] {
    const suggestions: any[] = []
    
    // Check if we found JavaScript-heavy content
    const hasJavaScript = pages.some(p => 
      p.technologies?.some(t => ['React', 'Vue', 'Angular', 'Next.js'].includes(t))
    )
    
    if (hasJavaScript) {
      suggestions.push({
        action: 'use_scraper',
        scraperId: 'dynamic',
        label: 'Use JavaScript renderer',
        reason: 'Detected JavaScript frameworks that may require client-side rendering',
        estimatedTime: '20-30s',
        estimatedValue: 'high',
        confidence: 85
      })
    }
    
    // Check for API endpoints
    const apiEndpoints = pages.flatMap(p => p.apiEndpoints || [])
    if (apiEndpoints.length > 0) {
      suggestions.push({
        action: 'use_scraper',
        scraperId: 'api',
        label: 'Extract API data',
        reason: `Found ${apiEndpoints.length} potential API endpoints`,
        estimatedTime: '5-10s',
        estimatedValue: 'high',
        confidence: 75
      })
    }
    
    return suggestions
  }

  /**
   * Validate scraper results
   */
  async validate(result: ScraperResult): Promise<ValidationResult> {
    const issues: ValidationResult['issues'] = []
    let completeness = 0
    let quality = 0
    
    // Check success rate
    const successRate = result.stats.successRate
    if (successRate < 50) {
      issues.push({
        severity: 'error',
        field: 'success_rate',
        message: `Low success rate: ${successRate.toFixed(1)}%`
      })
    } else if (successRate < 80) {
      issues.push({
        severity: 'warning',
        field: 'success_rate',
        message: `Moderate success rate: ${successRate.toFixed(1)}%`
      })
    }
    
    // Check data extraction
    const avgDataPoints = result.stats.pagesSucceeded > 0 
      ? result.stats.dataPointsExtracted / result.stats.pagesSucceeded
      : 0
      
    if (avgDataPoints < 5) {
      issues.push({
        severity: 'warning',
        field: 'data_extraction',
        message: 'Low data extraction rate'
      })
    }
    
    // Calculate scores
    completeness = Math.min(100, (successRate + (avgDataPoints * 2)))
    quality = successRate
    
    return {
      isValid: issues.filter(i => i.severity === 'error').length === 0,
      completeness,
      quality,
      issues,
      suggestions: []
    }
  }

  /**
   * Check if this scraper can handle the URL
   */
  canHandle(url: string, existingData?: MergedPageData): boolean {
    // Static scraper can handle any HTTP(S) URL
    if (!url.startsWith('http://') && !url.startsWith('https://')) {
      return false
    }
    
    // If we have existing data showing heavy JS, maybe not ideal
    if (existingData?.technologies.some(t => 
      ['React', 'Vue', 'Angular'].includes(t)
    )) {
      return true // Can still handle, but might not be optimal
    }
    
    return true
  }

  /**
   * Estimate the value this scraper would add
   */
  estimateValue(urls: string[], existingData?: Map<string, MergedPageData>): ValueEstimate {
    const unscrapedUrls = urls.filter(url => !existingData?.has(url))
    
    return {
      expectedDataPoints: unscrapedUrls.length * 10, // Estimate 10 data points per page
      confidence: 80,
      valueAdds: [
        'Fast HTML extraction',
        'Link discovery',
        'Contact information',
        'Social media links',
        'Basic structured data'
      ],
      estimatedTime: `${Math.ceil(unscrapedUrls.length * 0.5)}s`
    }
  }
}