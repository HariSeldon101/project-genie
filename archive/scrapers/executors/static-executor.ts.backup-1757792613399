/**
 * Static Scraper Executor - The Fast Website Reader
 * 
 * Think of this like a speed reader for websites! It quickly looks at the HTML
 * (the code that makes up a webpage) and grabs all the important information
 * without needing to run any JavaScript (the interactive parts).
 * 
 * What it does:
 * - Reads websites super fast (like scanning a book instead of reading every word)
 * - Perfect for simple websites, blogs, and news sites
 * - Saves everything it finds to help understand the company better
 * 
 * Special features:
 * - Keeps track of everything it does (like breadcrumbs in a forest)
 * - Always tells you if something goes wrong (no hiding errors!)
 * - Works together with other parts of the system
 * 
 * @module static-executor
 */

import * as cheerio from 'cheerio'
import {
  ScraperResult,
  ScraperOptions,
  PageResult,
  ValidationResult,
  MergedPageData,
  ValueEstimate,
  ScrapingError,
  DiscoveredLink,
  ContactInfo,
  SocialLinks,
  FormData,
  ImageData,
  URLMetadata
} from '../additive/types'
import { permanentLogger } from '@/lib/utils/permanent-logger'
import { BaseScraperExecutor, ProgressCallback } from './base-executor'
import { SSEEventFactory, EventSource } from '@/lib/company-intelligence/utils/sse-event-factory'
import { SmartExtractor } from '@/lib/company-intelligence/extractors/smart-extractor'
import { BlogContentExtractor } from '@/lib/company-intelligence/extractors/blog-content-extractor'
import { socialMediaExtractor } from '../extractors/social-media-extractor'

/**
 * Static HTML scraper using Cheerio
 * Extends BaseScraperExecutor for DRY principle
 */
export class StaticScraperExecutor extends BaseScraperExecutor {
  id = 'static'
  name = 'Static HTML Scraper (Cheerio)'
  strategy = 'static' as const
  description = 'Fast HTML parsing without JavaScript execution. Best for static sites, blogs, and server-rendered content.'
  speed = 'fast' as const
  
  // Smart skipping settings - like checking if you've already done your homework!
  private readonly SKIP_THRESHOLD = 24 * 60 * 60 * 1000 // Don't re-check pages for 24 hours (that's 86,400,000 milliseconds!)
  private readonly enableSmartSkipping = true // Turn on the smart "already did this" checker
  
  private smartExtractor: SmartExtractor
  private blogExtractor: BlogContentExtractor
  private socialExtractor: typeof socialMediaExtractor

  constructor() {
    super()
    this.smartExtractor = new SmartExtractor()
    this.blogExtractor = new BlogContentExtractor()
    this.socialExtractor = socialMediaExtractor
  }

  /**
   * The main function that reads websites and saves what it finds
   * Like a robot that visits websites and takes notes about everything it sees!
   */
  async scrape(urls: string[], options?: ScraperOptions): Promise<ScraperResult> {
    const startTime = Date.now()
    const pages: PageResult[] = []
    const allDiscoveredLinks: DiscoveredLink[] = []
    const errors: ScrapingError[] = []
    
    // First, let's check if we got any websites to look at
    permanentLogger.log(' STATIC_EXECUTOR.scrape CALLED:', {
      urlsReceived: !!urls,
      urlCount: urls ? urls.length : 0,
      urlsSample: urls ? urls.slice(0, 3) : [],
      urlsType: Array.isArray(urls) ? 'array' : typeof urls,
      hasOptions: !!options,
      sessionId: options?.sessionId
    })
    
    // If we don't have any websites to visit, we need to stop and tell someone!
    if (!urls || urls.length === 0) {
      // Log comprehensive error with forensic data
      permanentLogger.error('STATIC_EXECUTOR', 'No URLs provided for scraping', {
        breadcrumbs: permanentLogger.getBreadcrumbs(),
        timestamp: Date.now(),
        sessionId: options?.sessionId,
        urlsReceived: urls,
        urlsType: Array.isArray(urls) ? 'array' : typeof urls,
        source: 'static_executor_scrape',
        systemState: {
          hasOptions: !!options,
          optionKeys: options ? Object.keys(options) : []
        }
      })
      
      // Tell the user interface (the part you see on screen) that something went wrong
      if (options?.progressCallback) {
        const errorEvent = SSEEventFactory.error(
          new Error('No URLs provided for scraping - check sitemap discovery phase'),
          {
            source: EventSource.SCRAPER,
            phase: 'initialization',
            correlationId: options.sessionId
          }
        )
        
        try {
          await options.progressCallback(errorEvent)
        } catch (sseError) {
          permanentLogger.captureError('STATIC_EXECUTOR', error, {
        message: 'Failed to send SSE error event',
        error: sseError,
        errorMessage: error instanceof Error ? sseError.message : String(sseError)
      })
        }
      }
      
      // Stop everything and explain what went wrong (don't pretend it worked!)
      const noUrlError = new Error(
        'No URLs provided for scraping. Please ensure sitemap discovery has completed and discovered_urls contains valid URLs.'
      )
      
      // Add extra information to help figure out what happened
      (noUrlError as any).context = {
        sessionId: options?.sessionId,
        timestamp: Date.now(),
        source: 'StaticScraperExecutor',
        phase: 'initialization'
      }
      
      throw noUrlError
    }
    
    permanentLogger.breadcrumb('static_scraper_init', 'Static scraper initializing', {
      urlCount: urls.length,
      maxPages: options?.maxPages,
      timeout: options?.timeout,
      smartSkippingEnabled: this.enableSmartSkipping
    })
    
    // Smart skipping: Don't visit websites we just looked at (like not re-reading the same book page)
    let filteredUrls = [...urls]
    let skippedCount = 0
    const skipReasons: Record<string, number> = {
      recentlyScraped: 0,
      unchanged: 0,
      cached: 0
    }
    
    if (this.enableSmartSkipping && options?.urlMetadata && options.urlMetadata.size > 0) {
      permanentLogger.breadcrumb('smart_skipping', 'Applying smart skip filters', {
        urlCount: urls.length,
        skipThreshold: this.SKIP_THRESHOLD,
        enabled: this.enableSmartSkipping
      })
      
      const now = Date.now()
      filteredUrls = urls.filter(url => {
        const metadata = options.urlMetadata?.get(url)
        
        // Check if we already looked at this website recently
        if (metadata?.lastScraped) {
          const timeSinceLastScrape = now - metadata.lastScraped
          if (timeSinceLastScrape < this.SKIP_THRESHOLD) {
            permanentLogger.breadcrumb('skip_recent', 'Skipping recently scraped URL', {
              url,
              lastScraped: new Date(metadata.lastScraped).toISOString(),
              timeSinceLastScrape,
              threshold: this.SKIP_THRESHOLD
            })
            skippedCount++
            skipReasons.recentlyScraped++
            return false // Skip this URL
          }
        }
        
        // Check if the website changed since we last looked at it
        if (metadata?.lastScraped && metadata?.lastmod) {
          const lastScrapedTime = metadata.lastScraped
          const lastModTime = new Date(metadata.lastmod).getTime()
          
          if (lastScrapedTime > lastModTime) {
            permanentLogger.breadcrumb('skip_unchanged', 'Skipping unchanged page', {
              url,
              lastScraped: new Date(lastScrapedTime).toISOString(),
              lastModified: metadata.lastmod,
              hasBeenModified: false
            })
            skippedCount++
            skipReasons.unchanged++
            return false // Skip unchanged pages
          }
        }
        
        return true // Don't skip this URL
      })
      
      permanentLogger.info('Smart skip filter applied', { category: 'SMART_SKIPPING', ...{
        originalCount: urls.length,
        filteredCount: filteredUrls.length,
        skippedCount,
        skipReasons,
        percentSkipped: Math.round((skippedCount / urls.length }) * 100)
      })
    }
    
    // Sort websites by importance (like doing homework - math first, then reading!)
    let sortedUrls = [...filteredUrls]
    if (options?.urlMetadata && options.urlMetadata.size > 0) {
      permanentLogger.breadcrumb('url_prioritization', 'Sorting URLs by priority', {
        urlCount: urls.length,
        metadataCount: options.urlMetadata.size,
        sessionId: options?.sessionId
      })
      
      // Sort URLs by priority (high > medium > low > undefined)
      const priorityOrder = { 'high': 3, 'medium': 2, 'low': 1, undefined: 0 }
      sortedUrls = urls.sort((a, b) => {
        const metaA = options.urlMetadata?.get(a)
        const metaB = options.urlMetadata?.get(b)
        const priorityA = priorityOrder[metaA?.priority || 'low']
        const priorityB = priorityOrder[metaB?.priority || 'low']
        
        // Sort by priority first
        if (priorityA !== priorityB) {
          return priorityB - priorityA // Higher priority first
        }
        
        // If same priority, sort by lastmod (newer first)
        if (metaA?.lastmod && metaB?.lastmod) {
          return new Date(metaB.lastmod).getTime() - new Date(metaA.lastmod).getTime()
        }
        
        // If same priority, check page type importance
        const pageTypeOrder = { 
          'homepage': 5, 
          'about': 4, 
          'services': 3, 
          'products': 3,
          'blog': 2, 
          'contact': 1,
          undefined: 0 
        }
        const typeA = pageTypeOrder[metaA?.pageType as keyof typeof pageTypeOrder] || 0
        const typeB = pageTypeOrder[metaB?.pageType as keyof typeof pageTypeOrder] || 0
        
        return typeB - typeA
      })
      
      permanentLogger.info('URLs sorted by priority and metadata', { category: 'STATIC_EXECUTOR', ...{
        highPriorityCount: sortedUrls.filter(u => options.urlMetadata?.get(u })?.priority === 'high').length,
        mediumPriorityCount: sortedUrls.filter(u => options.urlMetadata?.get(u)?.priority === 'medium').length,
        lowPriorityCount: sortedUrls.filter(u => options.urlMetadata?.get(u)?.priority === 'low').length,
        noPriorityCount: sortedUrls.filter(u => !options.urlMetadata?.get(u)?.priority).length,
        firstUrl: sortedUrls[0],
        firstUrlPriority: options.urlMetadata?.get(sortedUrls[0])?.priority,
        sessionId: options?.sessionId
      })
    }
    
    // Initialize session with base class logging
    await this.startScrapingSession(sortedUrls, options)
    
    permanentLogger.info('Starting static HTML scraping', { category: 'STATIC_EXECUTOR', urlCount: sortedUrls.length,
      strategy: this.strategy,
      speed: this.speed,
      userAgent: options?.userAgent,
      hasHeaders: !!options?.headers,
      extractTypes: options?.extractTypes })

    // CRITICAL DEBUG: About to start processing URLs
    permanentLogger.log(' STATIC_EXECUTOR: About to process URLs in loop:', {
      urlCount: sortedUrls.length,
      firstUrl: sortedUrls[0],
      lastUrl: sortedUrls[sortedUrls.length - 1]
    })

    // Process URLs with detailed logging for each page
    let processedCount = 0
    for (const url of sortedUrls) {
      const urlStartTime = Date.now() // Track timing for this URL
      console.log(`ðŸ”´ STATIC_EXECUTOR: Processing URL ${processedCount + 1}/${sortedUrls.length}: ${url}`)
      
      // Add comprehensive logging at URL processing start
      permanentLogger.breadcrumb('url_processing_start', 'Starting URL processing', {
        url,
        index: processedCount,
        total: sortedUrls.length,
        sessionId: options?.sessionId,
        timestamp: urlStartTime
      })
      
      try {
        permanentLogger.breadcrumb('page_scrape_start', `Starting scrape of ${url}`, {
          url,
          index: processedCount,
          total: sortedUrls.length
        })
        
        // Log page attempt
        this.logPageAttempt(url, processedCount, sortedUrls.length)
        
        // Send progress event
        await this.sendProgressEvent(
          processedCount,
          sortedUrls.length,
          `Scraping ${url.substring(0, 50)}...`
        )
        
        const pageStartTime = Date.now()
        
        // CRITICAL DIAGNOSTIC LOGGING
        console.log(`ðŸ”´ STATIC_EXECUTOR: About to scrape page ${processedCount}/${sortedUrls.length}: ${url}`)
        
        const pageResult = await this.scrapePage(url, options)
        
        // CRITICAL DIAGNOSTIC LOGGING
        console.log(`ðŸ”´ STATIC_EXECUTOR: Page scrape result for ${url}:`, {
          success: pageResult.success,
          hasError: !!pageResult.error,
          error: pageResult.error,
          hasContent: !!pageResult.content,
          contentLength: pageResult.content?.length || 0
        })
        
        const pageDuration = Date.now() - pageStartTime
        
        pages.push(pageResult)
        
        if (pageResult.success) {
          // Log successful scrape
          await this.logPageSuccess(url, pageResult, pageDuration)
          
          // Collect discovered links with metadata
          if (pageResult.discoveredLinks) {
            permanentLogger.breadcrumb('links_discovered', `Found ${pageResult.discoveredLinks.length} links`, {
              url,
              linkCount: pageResult.discoveredLinks.length
            })
            
            for (const link of pageResult.discoveredLinks) {
              allDiscoveredLinks.push({
                url: link,
                foundOn: url,
                type: this.classifyLink(link, url),
                scraped: false
              })
            }
          }
        } else {
          // Log page error
          await this.logPageError(url, pageResult.error || 'Unknown error', pageDuration)
        }
        
        processedCount++
        
      } catch (error) {
        const errorMessage = error instanceof Error ? error.message : String(error) || 'Unknown error'
        const errorTimestamp = Date.now()
        const elapsedTime = errorTimestamp - startTime
        
        // Add breadcrumb for error tracking
        permanentLogger.breadcrumb('page_error', 'Failed to scrape page', {
          url,
          error: errorMessage,
          processedCount,
          totalUrls: urls.length,
          elapsedTime
        })
        
        // Log comprehensive error with breadcrumbs and timing
        permanentLogger.captureError('STATIC_EXECUTOR', error, {
        message: 'Critical error scraping page',
        url,
          Message,
          stack: error,
        errorMessage: error instanceof Error ? error.stack : undefined,
          processedCount,
          totalUrls: urls.length,
          elapsedTime,
          breadcrumbs: permanentLogger.getBreadcrumbs(),
          timestamp: errorTimestamp
      })
        
        // Log page error with timing
        await this.logPageError(url, error, elapsedTime)
        
        // Track error for reporting
        errors.push({
          code: 'SCRAPE_ERROR',
          message: errorMessage,
          url,
          stack: error instanceof Error ? error.stack : undefined,
          timestamp: errorTimestamp
        })
        
        // CRITICAL FIX: Add failed page result to pages array with ALL required fields
        // This ensures failed pages are counted in statistics and visible in UI
        pages.push({
          url,
          success: false,
          statusCode: (error as any).response?.status || (error as any).code === 'ENOTFOUND' ? 404 : 0,
          title: '',
          description: '',
          content: '',
          textContent: '',
          discoveredLinks: [],
          structuredData: {},
          technologies: [],
          apiEndpoints: [],
          contactInfo: { emails: [], phones: [], addresses: [] },
          socialLinks: {},
          forms: [],
          images: [],
          error: errorMessage,
          duration: Date.now() - urlStartTime,
          bytesDownloaded: 0,
          timestamp: errorTimestamp,
          // Additional error context for debugging
          errorDetails: {
            message: errorMessage,
            stack: error instanceof Error ? error.stack : undefined,
            code: (error as any).code || 'UNKNOWN_ERROR',
            phase: 'page_fetch'
          }
        } as PageResult)
        
        // Continue processing other URLs
        processedCount++
      }
    }

    // Calculate statistics using base class
    this.stats.duration = Date.now() - startTime
    this.stats.pagesAttempted = pages.length
    this.stats.pagesSucceeded = pages.filter(p => p.success).length
    this.stats.pagesFailed = pages.filter(p => !p.success).length
    this.stats.bytesDownloaded = pages.reduce((sum, p) => sum + (p.bytesDownloaded || 0), 0)
    this.stats.linksDiscovered = allDiscoveredLinks.length
    this.stats.averageTimePerPage = this.stats.pagesAttempted > 0 
      ? Math.round(this.stats.duration / this.stats.pagesAttempted) 
      : 0
    this.stats.successRate = this.stats.pagesAttempted > 0
      ? Math.round((this.stats.pagesSucceeded / this.stats.pagesAttempted) * 100)
      : 0
    
    // Count total data points
    for (const page of pages) {
      if (!page.success) continue
      let dataPoints = 0
      if (page.title) dataPoints++
      if (page.description) dataPoints++
      if (page.structuredData && Object.keys(page.structuredData).length > 0) {
        dataPoints += Object.keys(page.structuredData).length
      }
      if (page.technologies?.length) dataPoints += page.technologies.length
      if (page.contactInfo?.emails?.length) dataPoints += page.contactInfo.emails.length
      if (page.contactInfo?.phones?.length) dataPoints += page.contactInfo.phones.length
      if (page.socialLinks && Object.keys(page.socialLinks).length > 0) {
        dataPoints += Object.keys(page.socialLinks).length
      }
      if (page.forms?.length) dataPoints += page.forms.length
      if (page.discoveredLinks?.length) dataPoints += page.discoveredLinks.length
      this.stats.dataPointsExtracted += dataPoints
    }
    
    // Generate suggestions based on findings
    const suggestions = this.generateSuggestions(pages, allDiscoveredLinks)
    
    const result: ScraperResult = {
      scraperId: this.id,
      scraperName: this.name,
      strategy: this.strategy,
      timestamp: Date.now(),
      pages,
      discoveredLinks: allDiscoveredLinks,
      stats: this.stats,
      errors,
      suggestions
    }
    
    // Complete session with base class logging
    await this.completeScrapingSession(result, Date.now() - startTime)
    
    permanentLogger.info('Static scraping completed', { category: 'STATIC_EXECUTOR', duration: this.stats.duration,
      pagesSucceeded: this.stats.pagesSucceeded,
      pagesFailed: this.stats.pagesFailed,
      dataPointsExtracted: this.stats.dataPointsExtracted,
      linksDiscovered: allDiscoveredLinks.length,
      errors: errors.length,
      suggestions: suggestions.length })
    
    return result
  }

  /**
   * Scrape a single page with comprehensive logging
   */
  private async scrapePage(url: string, options?: ScraperOptions): Promise<PageResult> {
    const startTime = Date.now()
    
    permanentLogger.breadcrumb('fetch_start', `Fetching HTML for ${url}`, {
      url,
      timeout: options?.timeout,
      hasHeaders: !!options?.headers
    })
    
    try {
      // Fetch the HTML
      // Add comprehensive fetch logging
      permanentLogger.timing('fetch_attempt')
      permanentLogger.info('Starting HTTP fetch', { category: 'FETCH_START', ...{
        url,
        method: 'GET',
        timeout: options?.timeout || 'none',
        userAgent: options?.userAgent || 'default',
        timestamp: Date.now( })
      })
      
      const response = await fetch(url, {
        headers: options?.headers || {
          'User-Agent': options?.userAgent || 'Mozilla/5.0 (compatible; ProjectGenie/1.0)'
        },
        signal: options?.timeout ? AbortSignal.timeout(options.timeout) : undefined
      })
      
      permanentLogger.breadcrumb('fetch_complete', `Received response for ${url}`, {
        url,
        status: response.status,
        statusText: response.statusText,
        contentType: response.headers.get('content-type')
      })
      
      if (!response.ok) {
        const errorMessage = `HTTP ${response.status}: ${response.statusText}`
        
        // Log error with proper breadcrumbs for debugging
        permanentLogger.breadcrumb('http_error', `HTTP error for ${url}`, {
          url,
          status: response.status,
          statusText: response.statusText,
          is404: response.status === 404
        })
        
        permanentLogger.captureError('STATIC_EXECUTOR', new Error('HTTP error fetching page'), {
          url,
          status: response.status,
          statusText: response.statusText,
          errorMessage,
          is404: response.status === 404
        })
        
        // Following DRY/SOLID principles - throw error properly, no special structures
        // This allows proper error propagation and debugging
        throw new Error(errorMessage)
      }
      
      const html = await response.text()
      const htmlSize = new TextEncoder().encode(html).length
      
      // Log successful fetch with metrics
      permanentLogger.timing('fetch_success')
      permanentLogger.info('Successfully fetched HTML', { category: 'FETCH_COMPLETE', ...{
        url,
        statusCode: response.status,
        htmlLength: html.length,
        htmlSize,
        fetchDuration: Date.now( }) - startTime,
        bytesPerSecond: Math.round(htmlSize / ((Date.now() - startTime) / 1000))
      })
      
      permanentLogger.breadcrumb('html_received', `HTML received for ${url}`, {
        url,
        htmlSize,
        htmlLength: html.length
      })
      
      // Parse HTML with Cheerio
      const $ = cheerio.load(html)
      
      permanentLogger.breadcrumb('extraction_start', `Extracting data from ${url}`, {
        url
      })
      
      // Extract all data using both extractors and existing methods
      // Step 1: Use SmartExtractor for comprehensive company data
      const companyData = this.smartExtractor.extractFromHtml(html, url)
      permanentLogger.breadcrumb('smart_extraction_complete', 'SmartExtractor completed', {
        hasCompanyName: !!companyData.basics.companyName,
        hasDescription: !!companyData.basics.description,
        hasTagline: !!companyData.basics.tagline,
        productCount: companyData.business.products?.length || 0,
        serviceCount: companyData.business.services?.length || 0,
        teamSize: companyData.team.size,
        keyPeopleCount: companyData.team.keyPeople?.length || 0
      })
      
      // Step 2: Extract basic page data from extractors
      const title = companyData.basics.companyName || $('title').first().text().trim() || $('h1').first().text().trim()
      permanentLogger.breadcrumb('title_extracted', 'Title extracted', { title: title?.substring(0, 100) })
      
      const description = companyData.basics.description || 
        $('meta[name="description"]').attr('content') || 
        $('meta[property="og:description"]').attr('content') || ''
      permanentLogger.breadcrumb('description_extracted', 'Description extracted', { descLength: description?.length })
      
      // Extract text content directly (simplified version)
      const textContent = $('body').text().replace(/\s+/g, ' ').trim().substring(0, 50000)
      permanentLogger.breadcrumb('text_extracted', 'Text content extracted', { textLength: textContent?.length })
      
      // Step 3: Check if this is a blog page and use BlogContentExtractor
      const isBlogPage = url.includes('/blog') || url.includes('/news') || url.includes('/articles') || 
                        url.includes('/posts') || url.includes('/insights')
      let blogLinks: string[] = []
      
      if (isBlogPage) {
        blogLinks = this.blogExtractor.extractBlogLinks(html, url)
        permanentLogger.breadcrumb('blog_links_extracted', 'Blog-specific links extracted', { 
          blogLinkCount: blogLinks.length,
          isBlogPage: true 
        })
      }
      
      // Step 4: Extract all links directly
      const discoveredLinks: string[] = []
      $('a[href]').each((_, element) => {
        const href = $(element).attr('href')
        if (href) {
          try {
            const absoluteUrl = new URL(href, url).toString()
            if (absoluteUrl.startsWith('http')) {
              discoveredLinks.push(absoluteUrl)
            }
          } catch {
            // Invalid URL, skip
          }
        }
      })
      
      // Merge blog links with discovered links (avoiding duplicates)
      const allDiscoveredLinks = Array.from(new Set([...discoveredLinks, ...blogLinks]))
      permanentLogger.breadcrumb('links_extracted', 'All links extracted and merged', { 
        totalLinkCount: allDiscoveredLinks.length,
        blogLinkCount: blogLinks.length,
        regularLinkCount: discoveredLinks.length 
      })
      
      // Use our own inline implementation since StructuredDataExtractor has different signature
      const structuredData = this.extractStructuredData($)
      permanentLogger.breadcrumb('structured_data_extracted', 'Structured data extracted', { 
        hasStructuredData: Object.keys(structuredData).length > 0,
        types: Object.keys(structuredData)
      })
      
      const technologies = this.detectTechnologies($, html)
      permanentLogger.breadcrumb('technologies_detected', 'Technologies detected', { technologies })
      
      const apiEndpoints = this.extractAPIEndpoints($, html)
      permanentLogger.breadcrumb('api_endpoints_extracted', 'API endpoints extracted', { 
        endpointCount: apiEndpoints.length 
      })
      
      // Step 5: Use SmartExtractor data for contact info
      const contactInfo = {
        emails: companyData.basics.email ? [companyData.basics.email] : [],
        phones: companyData.basics.phone ? [companyData.basics.phone] : [],
        addresses: [] // SmartExtractor doesn't extract addresses yet
      }
      permanentLogger.breadcrumb('contact_info_extracted', 'Contact info extracted and enhanced', {
        emailCount: contactInfo.emails?.length || 0,
        phoneCount: contactInfo.phones?.length || 0,
        addressCount: contactInfo.addresses?.length || 0,
        fromSmartExtractor: !!(companyData.basics.email || companyData.basics.phone)
      })
      
      // Step 6: Use SmartExtractor social media data
      const socialLinks = companyData.basics.socialMedia || {}
      permanentLogger.breadcrumb('social_links_extracted', 'Social links extracted', {
        platforms: Object.keys(socialLinks)
      })
      
      const forms = this.extractForms($)
      permanentLogger.breadcrumb('forms_extracted', 'Forms extracted', { formCount: forms.length })
      
      const images = this.extractImages($, url)
      permanentLogger.breadcrumb('images_extracted', 'Images extracted', { imageCount: images.length })
      
      const duration = Date.now() - startTime
      
      permanentLogger.info('Page scraped successfully', { category: 'STATIC_EXECUTOR', ...{
        url,
        success: true,
        duration,
        statusCode: response.status,
        bytesDownloaded: htmlSize,
        title: title?.substring(0, 100 }),
        hasDescription: !!description,
        textLength: textContent?.length,
        linkCount: discoveredLinks.length,
        techCount: technologies.length,
        apiCount: apiEndpoints.length,
        formCount: forms.length,
        imageCount: images.length
      })
      
      return {
        url,
        success: true,
        statusCode: response.status,
        title,
        description,
        content: html,
        textContent,
        discoveredLinks: allDiscoveredLinks,
        structuredData,
        technologies,
        apiEndpoints,
        contactInfo,
        socialLinks,
        forms,
        images,
        duration,
        bytesDownloaded: htmlSize,
        // New data from extractors
        companyData,
        blogLinks,
        isBlogPage
      }
      
    } catch (error) {
      const errorMessage = error instanceof Error ? error.message : String(error)
      const duration = Date.now() - startTime

      permanentLogger.breadcrumb('page_scrape_error', `Failed to scrape ${url}`, {
        url,
        error: errorMessage,
        duration
      })

      permanentLogger.captureError('STATIC_EXECUTOR', error, {
        message: 'Failed to scrape page',
        url,
        Message,
        stack: error,
        errorMessage: error instanceof Error ? error.stack : undefined,
        duration
      })

      // CRITICAL FIX: Return partial data instead of throwing
      // This ensures the scraper doesn't completely fail and returns whatever data it could extract
      return {
        url,
        success: false,
        statusCode: 0,
        title: '',
        description: '',
        content: '',
        textContent: '',
        discoveredLinks: [],
        structuredData: {},
        technologies: [],
        apiEndpoints: [],
        contactInfo: { emails: [], phones: [], addresses: [] },
        socialLinks: {},
        forms: [],
        images: [],
        duration,
        bytesDownloaded: 0,
        error: errorMessage,
        errorDetails: {
          message: errorMessage,
          stack: error instanceof Error ? error.stack : undefined,
          phase: 'page_scrape'
        }
      } as PageResult
    }
  }


  /**
   * Extract forms
   */
  private extractForms($: cheerio.CheerioAPI): FormData[] {
    const forms: FormData[] = []
    
    $('form').each((_, element) => {
      const $form = $(element)
      const fields: FormData['fields'] = []
      
      $form.find('input, select, textarea').each((_, field) => {
        const $field = $(field)
        fields.push({
          name: $field.attr('name'),
          type: $field.attr('type') || $field.prop('tagName')?.toLowerCase(),
          required: $field.attr('required') !== undefined
        })
      })
      
      forms.push({
        action: $form.attr('action'),
        method: $form.attr('method'),
        fields
      })
    })
    
    return forms
  }

  /**
   * Extract images
   */
  private extractImages($: cheerio.CheerioAPI, baseUrl: string): ImageData[] {
    const images: ImageData[] = []
    const seen = new Set<string>()
    
    $('img').each((_, element) => {
      const $img = $(element)
      let src = $img.attr('src') || $img.attr('data-src')
      
      if (!src || seen.has(src)) return
      
      // Resolve relative URLs
      try {
        const url = new URL(src, baseUrl)
        src = url.href
      } catch {
        // Keep as is if invalid
      }
      
      seen.add(src)
      
      images.push({
        src,
        alt: $img.attr('alt'),
        width: parseInt($img.attr('width') || '0'),
        height: parseInt($img.attr('height') || '0')
      })
    })
    
    return images
  }

  /**
   * Classify a link type
   */
  private classifyLink(link: string, sourceUrl: string): DiscoveredLink['type'] {
    try {
      const linkUrl = new URL(link)
      const sourceUrlObj = new URL(sourceUrl)
      
      // Check if same domain
      if (linkUrl.hostname === sourceUrlObj.hostname) {
        // Check if it's an asset
        if (/\.(jpg|jpeg|png|gif|svg|css|js|pdf|doc|docx|xls|xlsx|zip)$/i.test(linkUrl.pathname)) {
          return 'asset'
        }
        // Check if it's an API endpoint
        if (linkUrl.pathname.includes('/api/') || linkUrl.pathname.includes('/v1/') || linkUrl.pathname.includes('/v2/')) {
          return 'api'
        }
        return 'internal'
      }
      
      // Check if it's a social media link
      if (/twitter\.com|facebook\.com|linkedin\.com|instagram\.com|youtube\.com|github\.com/i.test(linkUrl.hostname)) {
        return 'social'
      }
      
      return 'external'
    } catch {
      return 'external'
    }
  }

  /**
   * Generate suggestions based on findings
   */
  private generateSuggestions(pages: PageResult[], links: DiscoveredLink[]): any[] {
    const suggestions: any[] = []
    
    // Check if we found JavaScript-heavy content
    const hasJavaScript = pages.some(p => 
      p.technologies?.some(t => ['React', 'Vue', 'Angular', 'Next.js'].includes(t))
    )
    
    if (hasJavaScript) {
      suggestions.push({
        action: 'use_scraper',
        scraperId: 'dynamic',
        label: 'Use JavaScript renderer',
        reason: 'Detected JavaScript frameworks that may require client-side rendering',
        estimatedTime: '20-30s',
        estimatedValue: 'high',
        confidence: 85
      })
    }
    
    // Check for API endpoints
    const apiEndpoints = pages.flatMap(p => p.apiEndpoints || [])
    if (apiEndpoints.length > 0) {
      suggestions.push({
        action: 'use_scraper',
        scraperId: 'api',
        label: 'Extract API data',
        reason: `Found ${apiEndpoints.length} potential API endpoints`,
        estimatedTime: '5-10s',
        estimatedValue: 'high',
        confidence: 75
      })
    }
    
    return suggestions
  }

  /**
   * Validate scraper results
   */
  async validate(result: ScraperResult): Promise<ValidationResult> {
    const issues: ValidationResult['issues'] = []
    let completeness = 0
    let quality = 0
    
    // Check success rate
    const successRate = result.stats.successRate
    if (successRate < 50) {
      issues.push({
        severity: 'error',
        field: 'success_rate',
        message: `Low success rate: ${successRate.toFixed(1)}%`
      })
    } else if (successRate < 80) {
      issues.push({
        severity: 'warning',
        field: 'success_rate',
        message: `Moderate success rate: ${successRate.toFixed(1)}%`
      })
    }
    
    // Check data extraction
    const avgDataPoints = result.stats.pagesSucceeded > 0 
      ? result.stats.dataPointsExtracted / result.stats.pagesSucceeded
      : 0
      
    if (avgDataPoints < 5) {
      issues.push({
        severity: 'warning',
        field: 'data_extraction',
        message: 'Low data extraction rate'
      })
    }
    
    // Calculate scores
    completeness = Math.min(100, (successRate + (avgDataPoints * 2)))
    quality = successRate
    
    return {
      isValid: issues.filter(i => i.severity === 'error').length === 0,
      completeness,
      quality,
      issues,
      suggestions: []
    }
  }

  /**
   * Check if this scraper can handle the URL
   */
  canHandle(url: string, existingData?: MergedPageData): boolean {
    // Static scraper can handle any HTTP(S) URL
    if (!url.startsWith('http://') && !url.startsWith('https://')) {
      return false
    }
    
    // If we have existing data showing heavy JS, maybe not ideal
    if (existingData?.technologies.some(t => 
      ['React', 'Vue', 'Angular'].includes(t)
    )) {
      return true // Can still handle, but might not be optimal
    }
    
    return true
  }

  /**
   * Estimate the value this scraper would add
   */
  estimateValue(urls: string[], existingData?: Map<string, MergedPageData>): ValueEstimate {
    const unscrapedUrls = urls.filter(url => !existingData?.has(url))
    
    return {
      expectedDataPoints: unscrapedUrls.length * 10, // Estimate 10 data points per page
      confidence: 80,
      valueAdds: [
        'Fast HTML extraction',
        'Link discovery',
        'Contact information',
        'Social media links',
        'Basic structured data'
      ],
      estimatedTime: `${Math.ceil(unscrapedUrls.length * 0.5)}s`
    }
  }

  /**
   * Extract structured data from the page
   * Restoring this method that was accidentally removed in Phase 4
   */
  private extractStructuredData($: cheerio.CheerioAPI): any {
    const structuredData: any = {}
    
    permanentLogger.breadcrumb('structured_data_extraction_start', 'Starting structured data extraction')
    
    // Extract JSON-LD data
    $('script[type="application/ld+json"]').each((_, elem) => {
      try {
        const jsonLd = JSON.parse($(elem).html() || '{}')
        if (jsonLd['@type']) {
          structuredData.jsonLd = structuredData.jsonLd || []
          structuredData.jsonLd.push(jsonLd)
        }
      } catch {
        // Invalid JSON, skip
      }
    })
    
    // Extract OpenGraph data
    const ogData: any = {}
    $('meta[property^="og:"]').each((_, elem) => {
      const property = $(elem).attr('property')?.replace('og:', '')
      const content = $(elem).attr('content')
      if (property && content) {
        ogData[property] = content
      }
    })
    if (Object.keys(ogData).length > 0) {
      structuredData.openGraph = ogData
    }
    
    // Extract Twitter Card data
    const twitterData: any = {}
    $('meta[name^="twitter:"]').each((_, elem) => {
      const name = $(elem).attr('name')?.replace('twitter:', '')
      const content = $(elem).attr('content')
      if (name && content) {
        twitterData[name] = content
      }
    })
    if (Object.keys(twitterData).length > 0) {
      structuredData.twitterCard = twitterData
    }
    
    // Extract Schema.org microdata
    const schemaItems = $('[itemscope][itemtype]')
    if (schemaItems.length > 0) {
      structuredData.microdata = []
      schemaItems.each((_, elem) => {
        const itemType = $(elem).attr('itemtype')
        const props: any = {}
        $(elem).find('[itemprop]').each((_, propElem) => {
          const propName = $(propElem).attr('itemprop')
          const propValue = $(propElem).attr('content') || $(propElem).text().trim()
          if (propName) {
            props[propName] = propValue
          }
        })
        structuredData.microdata.push({
          type: itemType,
          properties: props
        })
      })
    }
    
    permanentLogger.breadcrumb('structured_data_extraction_complete', 'Structured data extraction complete', {
      hasJsonLd: !!structuredData.jsonLd,
      hasOpenGraph: !!structuredData.openGraph,
      hasTwitterCard: !!structuredData.twitterCard,
      hasMicrodata: !!structuredData.microdata
    })
    
    return structuredData
  }

  /**
   * Detect technologies used on the website
   * Restoring this method that was accidentally removed in Phase 4
   */
  private detectTechnologies($: cheerio.CheerioAPI, html: string): string[] {
    const technologies: string[] = []
    
    permanentLogger.breadcrumb('tech_detection_start', 'Starting technology detection')
    
    // Check for common frameworks in script tags
    $('script[src]').each((_, elem) => {
      const src = $(elem).attr('src') || ''
      
      // React
      if (src.includes('react') || src.includes('React')) technologies.push('React')
      // Vue
      if (src.includes('vue') || src.includes('Vue')) technologies.push('Vue')
      // Angular
      if (src.includes('angular') || src.includes('Angular')) technologies.push('Angular')
      // jQuery
      if (src.includes('jquery') || src.includes('jQuery')) technologies.push('jQuery')
      // Bootstrap
      if (src.includes('bootstrap')) technologies.push('Bootstrap')
      // Next.js
      if (src.includes('_next') || src.includes('next')) technologies.push('Next.js')
    })
    
    // Check for Tailwind CSS
    if (html.includes('tailwind') || html.includes('tw-')) technologies.push('Tailwind CSS')
    
    // Check meta generators
    const generator = $('meta[name="generator"]').attr('content')
    if (generator) technologies.push(generator)
    
    // Check for WordPress
    if (html.includes('wp-content') || html.includes('wp-includes')) {
      technologies.push('WordPress')
    }
    
    // Check for Shopify
    if (html.includes('shopify') || html.includes('Shopify')) {
      technologies.push('Shopify')
    }
    
    // Check for Webflow
    if (html.includes('webflow') || html.includes('Webflow')) {
      technologies.push('Webflow')
    }
    
    // Check for Squarespace
    if (html.includes('squarespace') || html.includes('Squarespace')) {
      technologies.push('Squarespace')
    }
    
    // Check for Wix
    if (html.includes('wix') || html.includes('Wix')) {
      technologies.push('Wix')
    }
    
    permanentLogger.breadcrumb('tech_detection_complete', 'Technology detection complete', {
      technologiesFound: technologies.length,
      technologies: technologies.slice(0, 5) // Log first 5 for brevity
    })
    
    return [...new Set(technologies)] // Remove duplicates
  }

  /**
   * Extract API endpoints from the HTML and JavaScript
   * Restoring this method that was accidentally removed in Phase 4
   */
  private extractAPIEndpoints($: cheerio.CheerioAPI, html: string): string[] {
    const endpoints: string[] = []
    
    permanentLogger.breadcrumb('api_extraction_start', 'Starting API endpoint extraction')
    
    // Pattern matching for API endpoints
    const apiPatterns = [
      /['"]([^'"]*\/api\/[^'"]+)['"]/g,
      /fetch\(['"]([^'"]+)['"]\)/g,
      /axios\.[get|post|put|delete]+\(['"]([^'"]+)['"]\)/g,
      /\.ajax\({[^}]*url:\s*['"]([^'"]+)['"]/g,
      /XMLHttpRequest.*open\([^,]+,\s*['"]([^'"]+)['"]/g,
      /graphql|GraphQL/gi,
      /\/v\d+\//g, // Versioned APIs like /v1/, /v2/
      /endpoint['":\s]+['"]([^'"]+)['"]/gi
    ]
    
    apiPatterns.forEach(pattern => {
      const matches = html.matchAll(pattern)
      for (const match of matches) {
        if (match[1] && !match[1].includes('example.com')) {
          endpoints.push(match[1])
        }
      }
    })
    
    // Check data attributes for API endpoints
    $('[data-api-endpoint], [data-api-url], [data-endpoint], [data-api]').each((_, elem) => {
      const endpoint = $(elem).attr('data-api-endpoint') || 
                      $(elem).attr('data-api-url') || 
                      $(elem).attr('data-endpoint') ||
                      $(elem).attr('data-api')
      if (endpoint) endpoints.push(endpoint)
    })
    
    // Check for API references in script tags
    $('script:not([src])').each((_, elem) => {
      const scriptContent = $(elem).html() || ''
      
      // Look for API configuration objects
      const apiConfigPattern = /api['":\s]+['"]([^'"]+)['"]/gi
      const configMatches = scriptContent.matchAll(apiConfigPattern)
      for (const match of configMatches) {
        if (match[1] && !match[1].includes('example.com')) {
          endpoints.push(match[1])
        }
      }
    })
    
    permanentLogger.breadcrumb('api_extraction_complete', 'API endpoint extraction complete', {
      endpointsFound: endpoints.length,
      sampleEndpoints: endpoints.slice(0, 3) // Log first 3 for brevity
    })
    
    return [...new Set(endpoints)] // Remove duplicates
  }
}