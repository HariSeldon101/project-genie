/**
 * Rate Limiter for LLM API Calls
 * Ensures we stay within 500 RPM limits for GPT-5 models
 */

import { LLMLogger } from '@/lib/utils/llm-logger'
import { permanentLogger } from '@/lib/utils/permanent-logger'

interface RateLimitConfig {
  maxRequestsPerMinute: number
  maxBurst?: number
  minSpacingMs?: number
}

interface RequestQueue {
  id: string
  fn: () => Promise<any>
  resolve: (value: any) => void
  reject: (error: any) => void
  timestamp: Date
  priority: number
}

interface SlidingWindow {
  requests: Date[]
  limit: number
  windowMs: number
}

export class RateLimiter {
  private static instance: RateLimiter
  private queues: Map<string, RequestQueue[]> = new Map()
  private windows: Map<string, SlidingWindow> = new Map()
  private processing: Map<string, boolean> = new Map()
  private lastRequestTime: Map<string, Date> = new Map()
  private retryAttempts: Map<string, number> = new Map()
  
  // Model-specific limits
  private readonly limits: Record<string, RateLimitConfig> = {
    'gpt-5': { maxRequestsPerMinute: 500, maxBurst: 10, minSpacingMs: 150 },
    'gpt-5-mini': { maxRequestsPerMinute: 500, maxBurst: 10, minSpacingMs: 150 },
    'gpt-5-nano': { maxRequestsPerMinute: 500, maxBurst: 10, minSpacingMs: 150 },
    'gpt-4.1-mini': { maxRequestsPerMinute: 500, maxBurst: 10, minSpacingMs: 150 },
    'gpt-4.1-nano': { maxRequestsPerMinute: 500, maxBurst: 10, minSpacingMs: 150 }
  }
  
  static getInstance(): RateLimiter {
    if (!this.instance) {
      this.instance = new RateLimiter()
    }
    return this.instance
  }
  
  /**
   * Execute a function with rate limiting
   */
  async executeWithRateLimit<T>(
    model: string,
    fn: () => Promise<T>,
    priority: number = 0
  ): Promise<T> {
    const config = this.limits[model] || { 
      maxRequestsPerMinute: 60, 
      maxBurst: 5, 
      minSpacingMs: 1000 
    }
    
    // Initialize window if needed
    if (!this.windows.has(model)) {
      this.windows.set(model, {
        requests: [],
        limit: config.maxRequestsPerMinute,
        windowMs: 60000 // 1 minute
      })
    }
    
    // Check if we can execute immediately
    const window = this.windows.get(model)!
    const now = new Date()
    
    // Clean old requests from window
    window.requests = window.requests.filter(
      time => now.getTime() - time.getTime() < window.windowMs
    )
    
    // Check rate limit
    const remaining = window.limit - window.requests.length
    const resetTime = new Date(
      window.requests[0] ? 
      window.requests[0].getTime() + window.windowMs : 
      now.getTime()
    )
    
    // Log rate limit status
    LLMLogger.logRateLimit(model, remaining, resetTime)
    
    // If we're at the limit, queue the request
    if (window.requests.length >= window.limit) {
      permanentLogger.info('Request queued due to rate limit', { category: 'RATE_LIMIT', ...{
        model,
        queueLength: this.getQueueLength(model }),
        remaining,
        resetTime
      })
      
      return this.queueRequest(model, fn, priority)
    }
    
    // Check minimum spacing
    const lastRequest = this.lastRequestTime.get(model)
    if (lastRequest && config.minSpacingMs) {
      const timeSinceLastRequest = now.getTime() - lastRequest.getTime()
      if (timeSinceLastRequest < config.minSpacingMs) {
        const waitTime = config.minSpacingMs - timeSinceLastRequest
        permanentLogger.log('RATE_LIMIT', `Waiting ${waitTime}ms for minimum spacing`, {
          model,
          waitTime
        })
        await this.delay(waitTime)
      }
    }
    
    // Execute the request
    window.requests.push(now)
    this.lastRequestTime.set(model, now)
    
    try {
      const result = await fn()
      this.retryAttempts.delete(`${model}-${fn.toString()}`)
      return result
    } catch (error: any) {
      // Handle rate limit errors
      if (error.status === 429 || error.code === 'rate_limit_exceeded') {
        return this.handleRateLimitError(model, fn, error, priority)
      }
      throw error
    } finally {
      // Process queued requests if any
      this.processQueue(model)
    }
  }
  
  /**
   * Handle rate limit errors with exponential backoff
   */
  private async handleRateLimitError<T>(
    model: string,
    fn: () => Promise<T>,
    error: any,
    priority: number
  ): Promise<T> {
    const attemptKey = `${model}-${fn.toString()}`
    const attempts = (this.retryAttempts.get(attemptKey) || 0) + 1
    this.retryAttempts.set(attemptKey, attempts)
    
    if (attempts > 3) {
      LLMLogger.logError(error, { model, phase: 'rate-limit', attemptNumber: attempts })
      throw new Error(`Max retry attempts exceeded for ${model}`)
    }
    
    // Calculate backoff delay
    const baseDelay = error.retryAfter ? error.retryAfter * 1000 : 1000
    const delay = baseDelay * Math.pow(2, attempts - 1) + Math.random() * 1000
    
    permanentLogger.log('RATE_LIMIT', `Rate limited, retrying after ${delay}ms`, {
      model,
      attempt: attempts,
      delay,
      error: error.message
    })
    
    LLMLogger.logError(error, { model, phase: 'rate-limit', attemptNumber: attempts })
    
    await this.delay(delay)
    
    // Retry with higher priority
    return this.executeWithRateLimit(model, fn, priority + 10)
  }
  
  /**
   * Queue a request for later execution
   */
  private queueRequest<T>(
    model: string,
    fn: () => Promise<T>,
    priority: number
  ): Promise<T> {
    return new Promise((resolve, reject) => {
      const request: RequestQueue = {
        id: Math.random().toString(36).substring(7),
        fn,
        resolve,
        reject,
        timestamp: new Date(),
        priority
      }
      
      if (!this.queues.has(model)) {
        this.queues.set(model, [])
      }
      
      const queue = this.queues.get(model)!
      queue.push(request)
      
      // Sort by priority (higher first) then by timestamp (older first)
      queue.sort((a, b) => {
        if (a.priority !== b.priority) {
          return b.priority - a.priority
        }
        return a.timestamp.getTime() - b.timestamp.getTime()
      })
      
      permanentLogger.info('Request queued', { category: 'RATE_LIMIT', model,
        queuePosition: queue.length,
        priority })
      
      // Start processing if not already
      this.processQueue(model)
    })
  }
  
  /**
   * Process queued requests
   */
  private async processQueue(model: string) {
    if (this.processing.get(model)) return
    
    const queue = this.queues.get(model)
    if (!queue || queue.length === 0) return
    
    this.processing.set(model, true)
    
    try {
      while (queue.length > 0) {
        const window = this.windows.get(model)!
        const now = new Date()
        
        // Clean old requests
        window.requests = window.requests.filter(
          time => now.getTime() - time.getTime() < window.windowMs
        )
        
        // Check if we can process
        if (window.requests.length >= window.limit) {
          // Wait for window to clear
          const waitTime = window.windowMs - (now.getTime() - window.requests[0].getTime())
          permanentLogger.log('RATE_LIMIT', `Queue waiting ${waitTime}ms for window to clear`, {
            model,
            queueLength: queue.length
          })
          await this.delay(waitTime)
          continue
        }
        
        // Process next request
        const request = queue.shift()!
        
        try {
          window.requests.push(now)
          this.lastRequestTime.set(model, now)
          
          const result = await request.fn()
          request.resolve(result)
        } catch (error) {
          request.reject(error)
        }
        
        // Minimum spacing between requests
        const config = this.limits[model]
        if (config?.minSpacingMs) {
          await this.delay(config.minSpacingMs)
        }
      }
    } finally {
      this.processing.set(model, false)
    }
  }
  
  /**
   * Get current queue length for a model
   */
  getQueueLength(model: string): number {
    return this.queues.get(model)?.length || 0
  }
  
  /**
   * Get rate limit status for a model
   */
  getRateLimitStatus(model: string): {
    remaining: number
    resetTime: Date
    queueLength: number
  } {
    const window = this.windows.get(model)
    const now = new Date()
    
    if (!window) {
      return {
        remaining: this.limits[model]?.maxRequestsPerMinute || 60,
        resetTime: now,
        queueLength: 0
      }
    }
    
    // Clean old requests
    window.requests = window.requests.filter(
      time => now.getTime() - time.getTime() < window.windowMs
    )
    
    return {
      remaining: window.limit - window.requests.length,
      resetTime: new Date(
        window.requests[0] ? 
        window.requests[0].getTime() + window.windowMs : 
        now.getTime()
      ),
      queueLength: this.getQueueLength(model)
    }
  }
  
  /**
   * Clear all queues and reset state
   */
  reset() {
    this.queues.clear()
    this.windows.clear()
    this.processing.clear()
    this.lastRequestTime.clear()
    this.retryAttempts.clear()
    
    permanentLogger.info('Rate limiter reset', { category: 'RATE_LIMIT' })
  }
  
  /**
   * Delay helper
   */
  private delay(ms: number): Promise<void> {
    return new Promise(resolve => setTimeout(resolve, ms))
  }
}

// Export singleton instance
export const rateLimiter = RateLimiter.getInstance()