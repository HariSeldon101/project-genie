/**
 * ScraperRegistryService - Auto-discovers and manages available scrapers
 * 
 * SOLID Principles Applied:
 * - Single Responsibility: Only manages scraper discovery and metadata
 * - Open/Closed: New scrapers auto-register without modifying this service
 * - Liskov Substitution: All scrapers implement same interface
 * 
 * DRY Principle: Centralizes scraper discovery and metadata
 */

import { permanentLogger } from '@/lib/utils/permanent-logger'
import { ReactNode } from 'react'
import { Zap, Globe, Code, Database } from 'lucide-react'

export interface ScraperMetadata {
  id: string
  name: string
  description: string
  icon: ReactNode | string
  speed: 'fast' | 'medium' | 'slow'
  capabilities: string[]
  disabled?: boolean
  used?: boolean
}

export class ScraperRegistryService {
  private scrapers = new Map<string, ScraperMetadata>()
  private static instance: ScraperRegistryService | null = null
  
  /**
   * Singleton pattern for global registry
   */
  static getInstance(): ScraperRegistryService {
    if (!ScraperRegistryService.instance) {
      ScraperRegistryService.instance = new ScraperRegistryService()
    }
    return ScraperRegistryService.instance
  }
  
  constructor() {
    this.autoDiscoverScrapers()
    
    permanentLogger.info('Service initialized', { category: 'SCRAPER_REGISTRY_SERVICE', scrapersDiscovered: this.scrapers.size })
  }
  
  /**
   * Auto-discover available scrapers
   * In production, this would scan the scrapers directory
   * For now, we register known scrapers
   */
  private autoDiscoverScrapers(): void {
    permanentLogger.breadcrumb('AUTO_DISCOVER', 'Starting scraper discovery')
    
    // Register Static HTML Scraper
    this.registerScraper({
      id: 'static',
      name: 'Static HTML (Cheerio)',
      description: 'Lightning-fast HTML extraction using Cheerio. Best for static websites with server-rendered content. Processes 5 pages in parallel.',
      icon: 'Zap',
      speed: 'fast',
      capabilities: [
        'HTML parsing',
        'Link extraction',
        'Meta tags',
        'Structured data',
        '10x speed'
      ],
      disabled: false
    })
    
    // Register Dynamic JavaScript Scraper
    this.registerScraper({
      id: 'dynamic',
      name: 'JavaScript Renderer (Playwright)',
      description: 'Full browser rendering with Playwright. Captures JavaScript-generated content, waits for AJAX calls, and handles SPAs. Slower but more complete.',
      icon: 'Globe',
      speed: 'medium',
      capabilities: [
        'JavaScript execution',
        'AJAX waiting',
        'Screenshots',
        'Form interaction',
        'Cookie handling'
      ],
      disabled: false
    })
    
    // Register SPA Scraper (Future)
    this.registerScraper({
      id: 'spa',
      name: 'SPA Scraper',
      description: 'Specialized for Single Page Applications. Advanced route detection and state management.',
      icon: 'Code',
      speed: 'slow',
      capabilities: [
        'Route detection',
        'State management',
        'Virtual DOM parsing',
        'Component extraction'
      ],
      disabled: true // Coming soon
    })
    
    // Register API Extractor (Future)
    this.registerScraper({
      id: 'api',
      name: 'API Extractor',
      description: 'Discovers and extracts data from API endpoints. Analyzes network traffic to find data sources.',
      icon: 'Database',
      speed: 'fast',
      capabilities: [
        'API discovery',
        'GraphQL introspection',
        'REST endpoint mapping',
        'Schema extraction'
      ],
      disabled: true // Coming soon
    })
    
    permanentLogger.info('Scrapers discovered', { category: 'SCRAPER_REGISTRY_SERVICE', ...{
      total: this.scrapers.size,
      enabled: Array.from(this.scrapers.values( })).filter(s => !s.disabled).length,
      scraperIds: Array.from(this.scrapers.keys())
    })
  }
  
  /**
   * Register a new scraper
   */
  private registerScraper(metadata: ScraperMetadata): void {
    this.scrapers.set(metadata.id, metadata)
    
    permanentLogger.breadcrumb('REGISTER_SCRAPER', 'Scraper registered', {
      id: metadata.id,
      name: metadata.name,
      disabled: metadata.disabled
    })
  }
  
  /**
   * Get scraper by ID
   */
  getScraperById(id: string): ScraperMetadata | undefined {
    return this.scrapers.get(id)
  }
  
  /**
   * Get all scrapers
   */
  getAllScrapers(): ScraperMetadata[] {
    return Array.from(this.scrapers.values())
  }
  
  /**
   * Get enabled scrapers only
   */
  getEnabledScrapers(): ScraperMetadata[] {
    return Array.from(this.scrapers.values()).filter(s => !s.disabled)
  }
  
  /**
   * Mark scraper as used
   */
  markScraperUsed(scraperId: string): void {
    const scraper = this.scrapers.get(scraperId)
    if (scraper) {
      scraper.used = true
      
      permanentLogger.breadcrumb('MARK_USED', 'Scraper marked as used', {
        scraperId,
        name: scraper.name
      })
    }
  }
  
  /**
   * Reset all scrapers to unused
   */
  resetUsedStatus(): void {
    for (const scraper of this.scrapers.values()) {
      scraper.used = false
    }
    
    permanentLogger.breadcrumb('RESET_USED', 'All scrapers reset to unused')
  }
  
  /**
   * Get scraper icon component
   * Maps string icon names to actual components
   */
  getScraperIcon(scraperOrId: string | ScraperMetadata): ReactNode {
    const scraper = typeof scraperOrId === 'string' 
      ? this.scrapers.get(scraperOrId)
      : scraperOrId
    
    if (!scraper) return null
    
    // Map string icons to components
    const iconMap: Record<string, ReactNode> = {
      'Zap': Zap,
      'Globe': Globe,
      'Code': Code,
      'Database': Database
    }
    
    if (typeof scraper.icon === 'string') {
      const IconComponent = iconMap[scraper.icon]
      return IconComponent || null
    }
    
    return scraper.icon
  }
  
  /**
   * Get scraper statistics
   */
  getStatistics(): {
    total: number
    enabled: number
    disabled: number
    used: number
  } {
    const scrapers = Array.from(this.scrapers.values())
    
    return {
      total: scrapers.length,
      enabled: scrapers.filter(s => !s.disabled).length,
      disabled: scrapers.filter(s => s.disabled).length,
      used: scrapers.filter(s => s.used).length
    }
  }
}